{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dd953f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\yanma\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\yanma\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tabulate import tabulate\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchmetrics.classification import ConfusionMatrix\n",
    "\n",
    "\n",
    "dataset_full = torchvision.datasets.FashionMNIST('data', train = True, download = True, transform = transforms.ToTensor())\n",
    "\n",
    "# dataset_full.classes\n",
    "idx_tshirt_trouser = [0, 1]\n",
    "idx_pullover_dress = [2, 3]\n",
    "idx_tshirt_trouser_pullover_dress = [0,1,2, 3]\n",
    "\n",
    "dataset_tshirt_trouser = [data for data in dataset_full if data[1] in idx_tshirt_trouser] # 01\n",
    "dataset_pullover_dress = [data for data in dataset_full if data[1] in idx_pullover_dress] # 23\n",
    "dataset_tshirt_trouser_pullover_dress = [data for data in dataset_full if data[1] in idx_tshirt_trouser_pullover_dress]\n",
    "\n",
    "\n",
    "train_dataset_all, test_dataset_all = torch.utils.data.dataset.random_split(dataset_full, [50000, 10000])\n",
    "train_dataset_01, test_dataset_01 = torch.utils.data.dataset.random_split(dataset_tshirt_trouser, [10000, 2000])\n",
    "train_dataset_23, test_dataset_23 = torch.utils.data.dataset.random_split(dataset_pullover_dress, [10000, 2000])\n",
    "train_dataset_0123, test_dataset_0123 = torch.utils.data.dataset.random_split(dataset_tshirt_trouser_pullover_dress, [20000, 4000])\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "train_loader_all = DataLoader(train_dataset_all, batch_size=batch_size)\n",
    "test_loader_all = DataLoader(test_dataset_all, batch_size=batch_size)\n",
    "\n",
    "train_loader_01 = DataLoader(train_dataset_01, batch_size=batch_size)\n",
    "test_loader_01 = DataLoader(test_dataset_01, batch_size=batch_size)\n",
    "\n",
    "train_loader_23 = DataLoader(train_dataset_23, batch_size=batch_size)\n",
    "test_loader_23 = DataLoader(test_dataset_23, batch_size=batch_size)\n",
    "\n",
    "train_loader_0123 = DataLoader(train_dataset_0123, batch_size=batch_size)\n",
    "test_loader_0123 = DataLoader(test_dataset_0123, batch_size=batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d10fdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FashionCNN, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=64 * 6 * 6, out_features=600)\n",
    "        # self.drop = nn.Dropout2d(0.25)\n",
    "        self.fc2 = nn.Linear(in_features=600, out_features=120)\n",
    "        self.fc3 = nn.Linear(in_features=120, out_features=4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        # out = self.drop(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ba6cd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "L01 = []\n",
    "L23 = []\n",
    "\n",
    "for i in range(50):\n",
    "    # Charger le mod√®le\n",
    "    model = FashionCNN()\n",
    "    model.load_state_dict(torch.load(fr'v2/model_01_0123_2layers_ex_{i}.pth')['model_state_dict'])\n",
    "    model.eval()\n",
    "    L01.append(model)\n",
    "    model = FashionCNN()\n",
    "    model.load_state_dict(torch.load(fr'v2/model_23_0123_2layers_ex_{i}.pth')['model_state_dict'])\n",
    "    model.eval()\n",
    "    L23.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a862906",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create an array to store all images\n",
    "images = np.zeros((784, 28, 28))\n",
    "\n",
    "for i in range(784):\n",
    "    image = np.zeros((28, 28))\n",
    "    row = i // 28\n",
    "    col = i % 28\n",
    "    image[row, col] = 1\n",
    "    images[i] = image\n",
    "\n",
    "idx = 600\n",
    "plt.imshow(images[idx], cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(f\"Image {idx}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63f4624",
   "metadata": {},
   "source": [
    "## Cr√©ations d'images dataset mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81af1ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tabulate import tabulate\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchmetrics.classification import ConfusionMatrix\n",
    "\n",
    "import os\n",
    "import struct\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "182ffb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hybrid(image1, image2):\n",
    "\n",
    "    image_width = image1.shape[1]\n",
    "    image_height = image1.shape[0]\n",
    "    kernel_size = (int(image_height/5), int(image_width/5))\n",
    "\n",
    "    image2 = cv2.resize(image2, (image_width, image_height))\n",
    "\n",
    "    # Apply a low-pass filter (Gaussian blur) to image 1\n",
    "    # Apply a high-pass filter (subtract blurred image from original) to image 2\n",
    "    image1_blur = cv2.GaussianBlur(image1, kernel_size, 0)\n",
    "    image1_highpass = image1 - image1_blur\n",
    "\n",
    "    # Apply a low-pass filter (Gaussian blur) to image 2\n",
    "    # Apply a high-pass filter (subtract blurred image from original) to image 2\n",
    "    image2_blur = cv2.GaussianBlur(image2, kernel_size, 0)\n",
    "    image2_highpass = image2 - image2_blur\n",
    "\n",
    "    # Combine the low-frequency component of image 1 with the high-frequency component of image 2\n",
    "    hybrid_image1 = image1_blur + image2_highpass\n",
    "\n",
    "    # Combine the low-frequency component of image 2 with the high-frequency component of image 1\n",
    "    hybrid_image2 = image2_blur + image1_highpass\n",
    "\n",
    "    # Divid all images pixels by a value and add them together\n",
    "    image1 = image1/3\n",
    "    image2 = image2/3\n",
    "    hybrid_image3 = image1 + image2\n",
    "\n",
    "    return hybrid_image1, hybrid_image2, hybrid_image3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ac244cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to FashionMNIST dataset\n",
    "\n",
    "dataset_full = torchvision.datasets.FashionMNIST('data', train = True, download = True, transform = transforms.ToTensor())\n",
    "\n",
    "\n",
    "# dataset_full.classes\n",
    "idx_0 = [0]\n",
    "idx_1 = [1]\n",
    "idx_2 = [2]\n",
    "idx_3 = [3]\n",
    "idx_pullover_dress = [2, 3]\n",
    "idx_tshirt_trouser_pullover_dress = [0,1,2, 3]\n",
    "\n",
    "dataset_0 = [data for data in dataset_full if data[1] in idx_0] \n",
    "dataset_1 = [data for data in dataset_full if data[1] in idx_1]\n",
    "dataset_2 = [data for data in dataset_full if data[1] in idx_2]\n",
    "dataset_3 = [data for data in dataset_full if data[1] in idx_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aeb83bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {0: dataset_0, 1: dataset_1, 2:dataset_2, 3:dataset_3}\n",
    "images = []\n",
    "for i in range(1):\n",
    "    for j in range(2,3):\n",
    "        for p in range(600):\n",
    "            images.append(create_hybrid(datasets[i][p][0].numpy().reshape(28, 28),datasets[j][p][0].numpy().reshape(28, 28))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801d9bd0",
   "metadata": {},
   "source": [
    "# Test part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd7227e",
   "metadata": {},
   "source": [
    "## Test with a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c32b404",
   "metadata": {},
   "source": [
    "#### Creation of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b7f0ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "L01 = []\n",
    "L23 = []\n",
    "\n",
    "for i in range(50):\n",
    "    # Charger le mod√®le\n",
    "    model = FashionCNN()\n",
    "    model.load_state_dict(torch.load(fr'v2/model_01_0123_2layers_ex_{i}.pth')['model_state_dict'])\n",
    "    model.eval()\n",
    "    L01.append(model)\n",
    "    model = FashionCNN()\n",
    "    model.load_state_dict(torch.load(fr'v2/model_23_0123_2layers_ex_{i}.pth')['model_state_dict'])\n",
    "    model.eval()\n",
    "    L23.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "694558fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa6c2490",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.zeros([100,601])\n",
    "data_test = np.zeros([20,601])\n",
    "for i in range(50):\n",
    "    model1 = FashionCNN()\n",
    "    model1.load_state_dict(torch.load(fr'v2/model_01_0123_2layers_ex_{i}.pth')['model_state_dict'])\n",
    "    model1.eval()\n",
    "    model2 = FashionCNN()\n",
    "    model2.load_state_dict(torch.load(fr'v2/model_23_0123_2layers_ex_{i}.pth')['model_state_dict'])\n",
    "    model2.eval()\n",
    "    for j in range(600):\n",
    "        data[i,j] = model1(torch.tensor(images[j], dtype = torch.float).unsqueeze(0).unsqueeze(0)).argmax(1).item()\n",
    "        data[i,600] = 0\n",
    "        data[i+40,j] = model2(torch.tensor(images[j], dtype = torch.float).unsqueeze(0).unsqueeze(0)).argmax(1).item()\n",
    "        data[i+40,600] = 1\n",
    "        \n",
    "        \n",
    "for i in range(50,60):\n",
    "    model1 = FashionCNN()\n",
    "    model1.load_state_dict(torch.load(fr'v2/model_01_0123_2layers_ex_{i}.pth')['model_state_dict'])\n",
    "    model1.eval()\n",
    "    model2 = FashionCNN()\n",
    "    model2.load_state_dict(torch.load(fr'v2/model_23_0123_2layers_ex_{i}.pth')['model_state_dict'])\n",
    "    model2.eval()\n",
    "    for j in range(600):\n",
    "        data_test[i-50,j] = model1(torch.tensor(images[j], dtype = torch.float).unsqueeze(0).unsqueeze(0)).argmax(1).item()\n",
    "        data_test[i-50,600] = 0\n",
    "        data_test[i+10-50,j] = model2(torch.tensor(images[j], dtype = torch.float).unsqueeze(0).unsqueeze(0)).argmax(1).item()\n",
    "        data_test[i+10-50,600] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1c61a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 601)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "adcfa552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 601)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a9ba87",
   "metadata": {},
   "source": [
    "NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c91f02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deep(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Deep, self).__init__()\n",
    "        self.fc1 = nn.Linear(600, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 300)\n",
    "        self.fc3 = nn.Linear(300, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 600)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    \n",
    "# Define model\n",
    "model_args = {}\n",
    "# random seed\n",
    "model_args['seed'] = 123\n",
    "# we will use batch size of 128 in Stochastic Gradient Descent (SGD) optimization of the network\n",
    "model_args['batch_size'] = 10\n",
    "# learning rate is how fast it will descend\n",
    "model_args['lr'] = .05\n",
    "# SGD momentum (default: .5) momentum is a moving average of gradients (it helps to keep direction)\n",
    "model_args['momentum'] = .5\n",
    "# the number of epochs is the number of times you go through the full dataset\n",
    "model_args['epochs'] = 30\n",
    "# logging frequency\n",
    "model_args['log_interval'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a78b55a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelDataset(Dataset):\n",
    "    def __init__(self, signals, annotations):\n",
    "        self.signals = signals\n",
    "        self.annotations = annotations\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.signals)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        signal = self.signals[idx]\n",
    "        annotation = self.annotations[idx]\n",
    "        return signal, annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8cc1fa8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 600) (100,)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, y_train = data[:,:-1], data[:,-1]\n",
    "X_test, y_test = data_test[:,:-1], data[:,-1]\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "# Convert data to PyTorch tensors and move to GPU\n",
    "X_train = torch.Tensor(X_train).to(device)\n",
    "y_train = torch.LongTensor(y_train).to(device)\n",
    "X_test = torch.Tensor(X_test).to(device)\n",
    "y_test = torch.LongTensor(y_test).to(device)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = PixelDataset(X_train, y_train)\n",
    "test_dataset = PixelDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=model_args['batch_size'], shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=model_args['batch_size'], shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e62db965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/100 (0%)]\tLoss: 0.701818\n",
      "Train Epoch: 1 [10/100 (10%)]\tLoss: 0.709077\n",
      "Train Epoch: 1 [20/100 (20%)]\tLoss: 0.581174\n",
      "Train Epoch: 1 [30/100 (30%)]\tLoss: 0.550521\n",
      "Train Epoch: 1 [40/100 (40%)]\tLoss: 0.399774\n",
      "Train Epoch: 1 [50/100 (50%)]\tLoss: 0.832717\n",
      "Train Epoch: 1 [60/100 (60%)]\tLoss: 0.338381\n",
      "Train Epoch: 1 [70/100 (70%)]\tLoss: 0.709760\n",
      "Train Epoch: 1 [80/100 (80%)]\tLoss: 0.495027\n",
      "Train Epoch: 1 [90/100 (90%)]\tLoss: 0.560351\n",
      "\n",
      "Test set: \n",
      "Average loss: 0.0628, \n",
      "Accuracy: 14/20 (70.00%)\n",
      "\n",
      "Train Epoch: 2 [0/100 (0%)]\tLoss: 0.186982\n",
      "Train Epoch: 2 [10/100 (10%)]\tLoss: 0.135142\n",
      "Train Epoch: 2 [20/100 (20%)]\tLoss: 0.650489\n",
      "Train Epoch: 2 [30/100 (30%)]\tLoss: 0.154013\n",
      "Train Epoch: 2 [40/100 (40%)]\tLoss: 0.240951\n",
      "Train Epoch: 2 [50/100 (50%)]\tLoss: 0.224706\n",
      "Train Epoch: 2 [60/100 (60%)]\tLoss: 0.266659\n",
      "Train Epoch: 2 [70/100 (70%)]\tLoss: 0.301374\n",
      "Train Epoch: 2 [80/100 (80%)]\tLoss: 0.433912\n",
      "Train Epoch: 2 [90/100 (90%)]\tLoss: 1.059508\n",
      "\n",
      "Test set: \n",
      "Average loss: 0.1072, \n",
      "Accuracy: 8/20 (40.00%)\n",
      "\n",
      "Train Epoch: 3 [0/100 (0%)]\tLoss: 0.174658\n",
      "Train Epoch: 3 [10/100 (10%)]\tLoss: 0.557567\n",
      "Train Epoch: 3 [20/100 (20%)]\tLoss: 0.271990\n",
      "Train Epoch: 3 [30/100 (30%)]\tLoss: 0.142286\n",
      "Train Epoch: 3 [40/100 (40%)]\tLoss: 0.180533\n",
      "Train Epoch: 3 [50/100 (50%)]\tLoss: 0.109943\n",
      "Train Epoch: 3 [60/100 (60%)]\tLoss: 0.168898\n",
      "Train Epoch: 3 [70/100 (70%)]\tLoss: 0.144594\n",
      "Train Epoch: 3 [80/100 (80%)]\tLoss: 0.139939\n",
      "Train Epoch: 3 [90/100 (90%)]\tLoss: 0.115658\n",
      "\n",
      "Test set: \n",
      "Average loss: 0.1833, \n",
      "Accuracy: 8/20 (40.00%)\n",
      "\n",
      "Train Epoch: 4 [0/100 (0%)]\tLoss: 0.094383\n",
      "Train Epoch: 4 [10/100 (10%)]\tLoss: 0.064216\n",
      "Train Epoch: 4 [20/100 (20%)]\tLoss: 0.108982\n",
      "Train Epoch: 4 [30/100 (30%)]\tLoss: 0.073802\n",
      "Train Epoch: 4 [40/100 (40%)]\tLoss: 0.242780\n",
      "Train Epoch: 4 [50/100 (50%)]\tLoss: 0.018932\n",
      "Train Epoch: 4 [60/100 (60%)]\tLoss: 0.392291\n",
      "Train Epoch: 4 [70/100 (70%)]\tLoss: 0.081132\n",
      "Train Epoch: 4 [80/100 (80%)]\tLoss: 0.460066\n",
      "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.033239\n",
      "\n",
      "Test set: \n",
      "Average loss: 0.1867, \n",
      "Accuracy: 9/20 (45.00%)\n",
      "\n",
      "Train Epoch: 5 [0/100 (0%)]\tLoss: 0.040984\n",
      "Train Epoch: 5 [10/100 (10%)]\tLoss: 0.059013\n",
      "Train Epoch: 5 [20/100 (20%)]\tLoss: 0.059696\n",
      "Train Epoch: 5 [30/100 (30%)]\tLoss: 0.054071\n",
      "Train Epoch: 5 [40/100 (40%)]\tLoss: 0.119763\n",
      "Train Epoch: 5 [50/100 (50%)]\tLoss: 0.026646\n",
      "Train Epoch: 5 [60/100 (60%)]\tLoss: 0.103182\n",
      "Train Epoch: 5 [70/100 (70%)]\tLoss: 0.059931\n",
      "Train Epoch: 5 [80/100 (80%)]\tLoss: 0.009702\n",
      "Train Epoch: 5 [90/100 (90%)]\tLoss: 0.109642\n",
      "\n",
      "Test set: \n",
      "Average loss: 0.1749, \n",
      "Accuracy: 10/20 (50.00%)\n",
      "\n",
      "Train Epoch: 6 [0/100 (0%)]\tLoss: 0.003523\n",
      "Train Epoch: 6 [10/100 (10%)]\tLoss: 0.087499\n",
      "Train Epoch: 6 [20/100 (20%)]\tLoss: 0.064326\n",
      "Train Epoch: 6 [30/100 (30%)]\tLoss: 0.044809\n",
      "Train Epoch: 6 [40/100 (40%)]\tLoss: 0.003550\n",
      "Train Epoch: 6 [50/100 (50%)]\tLoss: 0.081425\n",
      "Train Epoch: 6 [60/100 (60%)]\tLoss: 0.009609\n",
      "Train Epoch: 6 [70/100 (70%)]\tLoss: 0.050181\n",
      "Train Epoch: 6 [80/100 (80%)]\tLoss: 0.048167\n",
      "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.096120\n",
      "\n",
      "Test set: \n",
      "Average loss: 0.1958, \n",
      "Accuracy: 10/20 (50.00%)\n",
      "\n",
      "Train Epoch: 7 [0/100 (0%)]\tLoss: 0.115066\n",
      "Train Epoch: 7 [10/100 (10%)]\tLoss: 0.039573\n",
      "Train Epoch: 7 [20/100 (20%)]\tLoss: 0.005113\n",
      "Train Epoch: 7 [30/100 (30%)]\tLoss: 0.039462\n",
      "Train Epoch: 7 [40/100 (40%)]\tLoss: 0.075110\n",
      "Train Epoch: 7 [50/100 (50%)]\tLoss: 0.010642\n",
      "Train Epoch: 7 [60/100 (60%)]\tLoss: 0.080940\n",
      "Train Epoch: 7 [70/100 (70%)]\tLoss: 0.037222\n",
      "Train Epoch: 7 [80/100 (80%)]\tLoss: 0.002536\n",
      "Train Epoch: 7 [90/100 (90%)]\tLoss: 0.001004\n",
      "\n",
      "Test set: \n",
      "Average loss: 0.2397, \n",
      "Accuracy: 10/20 (50.00%)\n",
      "\n",
      "Train Epoch: 8 [0/100 (0%)]\tLoss: 0.070462\n",
      "Train Epoch: 8 [10/100 (10%)]\tLoss: 0.037321\n",
      "Train Epoch: 8 [20/100 (20%)]\tLoss: 0.037304\n",
      "Train Epoch: 8 [30/100 (30%)]\tLoss: 0.003404\n",
      "Train Epoch: 8 [40/100 (40%)]\tLoss: 0.032930\n",
      "Train Epoch: 8 [50/100 (50%)]\tLoss: 0.068068\n",
      "Train Epoch: 8 [60/100 (60%)]\tLoss: 0.034135\n",
      "Train Epoch: 8 [70/100 (70%)]\tLoss: 0.000886\n",
      "Train Epoch: 8 [80/100 (80%)]\tLoss: 0.069792\n",
      "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.001386\n",
      "\n",
      "Test set: \n",
      "Average loss: 0.2352, \n",
      "Accuracy: 10/20 (50.00%)\n",
      "\n",
      "Train Epoch: 9 [0/100 (0%)]\tLoss: 0.063615\n",
      "Train Epoch: 9 [10/100 (10%)]\tLoss: 0.032227\n",
      "Train Epoch: 9 [20/100 (20%)]\tLoss: 0.090930\n",
      "Train Epoch: 9 [30/100 (30%)]\tLoss: 0.006118\n",
      "Train Epoch: 9 [40/100 (40%)]\tLoss: 0.031669\n",
      "Train Epoch: 9 [50/100 (50%)]\tLoss: 0.029406\n",
      "Train Epoch: 9 [60/100 (60%)]\tLoss: 0.030300\n",
      "Train Epoch: 9 [70/100 (70%)]\tLoss: 0.001903\n",
      "Train Epoch: 9 [80/100 (80%)]\tLoss: 0.000756\n",
      "Train Epoch: 9 [90/100 (90%)]\tLoss: 0.031440\n",
      "\n",
      "Test set: \n",
      "Average loss: 0.2467, \n",
      "Accuracy: 10/20 (50.00%)\n",
      "\n",
      "Train Epoch: 10 [0/100 (0%)]\tLoss: 0.002564\n",
      "Train Epoch: 10 [10/100 (10%)]\tLoss: 0.056222\n",
      "Train Epoch: 10 [20/100 (20%)]\tLoss: 0.000951\n",
      "Train Epoch: 10 [30/100 (30%)]\tLoss: 0.000574\n",
      "Train Epoch: 10 [40/100 (40%)]\tLoss: 0.033604\n",
      "Train Epoch: 10 [50/100 (50%)]\tLoss: 0.027917\n",
      "Train Epoch: 10 [60/100 (60%)]\tLoss: 0.084640\n",
      "Train Epoch: 10 [70/100 (70%)]\tLoss: 0.053710\n",
      "Train Epoch: 10 [80/100 (80%)]\tLoss: 0.000851\n",
      "Train Epoch: 10 [90/100 (90%)]\tLoss: 0.026640\n",
      "\n",
      "Test set: \n",
      "Average loss: 0.2514, \n",
      "Accuracy: 10/20 (50.00%)\n",
      "\n",
      "Train Epoch: 11 [0/100 (0%)]\tLoss: 0.026087\n",
      "Train Epoch: 11 [10/100 (10%)]\tLoss: 0.026477\n",
      "Train Epoch: 11 [20/100 (20%)]\tLoss: 0.050733\n",
      "Train Epoch: 11 [30/100 (30%)]\tLoss: 0.000387\n",
      "Train Epoch: 11 [40/100 (40%)]\tLoss: 0.074112\n",
      "Train Epoch: 11 [50/100 (50%)]\tLoss: 0.024715\n",
      "Train Epoch: 11 [60/100 (60%)]\tLoss: 0.006041\n",
      "Train Epoch: 11 [70/100 (70%)]\tLoss: 0.001156\n",
      "Train Epoch: 11 [80/100 (80%)]\tLoss: 0.023934\n",
      "Train Epoch: 11 [90/100 (90%)]\tLoss: 0.026736\n",
      "\n",
      "Test set: \n",
      "Average loss: 0.2602, \n",
      "Accuracy: 10/20 (50.00%)\n",
      "\n",
      "Train Epoch: 12 [0/100 (0%)]\tLoss: 0.024582\n",
      "Train Epoch: 12 [10/100 (10%)]\tLoss: 0.000334\n",
      "Train Epoch: 12 [20/100 (20%)]\tLoss: 0.069386\n",
      "Train Epoch: 12 [30/100 (30%)]\tLoss: 0.024023\n",
      "Train Epoch: 12 [40/100 (40%)]\tLoss: 0.000952\n",
      "Train Epoch: 12 [50/100 (50%)]\tLoss: 0.004018\n",
      "Train Epoch: 12 [60/100 (60%)]\tLoss: 0.000355\n",
      "Train Epoch: 12 [70/100 (70%)]\tLoss: 0.045913\n",
      "Train Epoch: 12 [80/100 (80%)]\tLoss: 0.000706\n",
      "Train Epoch: 12 [90/100 (90%)]\tLoss: 0.067063\n",
      "\n",
      "Test set: \n",
      "Average loss: 0.2659, \n",
      "Accuracy: 10/20 (50.00%)\n",
      "\n",
      "Train Epoch: 13 [0/100 (0%)]\tLoss: 0.000172\n",
      "Train Epoch: 13 [10/100 (10%)]\tLoss: 0.022456\n",
      "Train Epoch: 13 [20/100 (20%)]\tLoss: 0.063756\n",
      "Train Epoch: 13 [30/100 (30%)]\tLoss: 0.022018\n",
      "Train Epoch: 13 [40/100 (40%)]\tLoss: 0.045561\n",
      "Train Epoch: 13 [50/100 (50%)]\tLoss: 0.000746\n",
      "Train Epoch: 13 [60/100 (60%)]\tLoss: 0.020451\n",
      "Train Epoch: 13 [70/100 (70%)]\tLoss: 0.021592\n",
      "Train Epoch: 13 [80/100 (80%)]\tLoss: 0.000303\n",
      "Train Epoch: 13 [90/100 (90%)]\tLoss: 0.019881\n",
      "\n",
      "Test set: \n",
      "Average loss: 0.2701, \n",
      "Accuracy: 10/20 (50.00%)\n",
      "\n",
      "Train Epoch: 14 [0/100 (0%)]\tLoss: 0.019759\n",
      "Train Epoch: 14 [10/100 (10%)]\tLoss: 0.020031\n",
      "Train Epoch: 14 [20/100 (20%)]\tLoss: 0.000161\n",
      "Train Epoch: 14 [30/100 (30%)]\tLoss: 0.000123\n",
      "Train Epoch: 14 [40/100 (40%)]\tLoss: 0.020390\n",
      "Train Epoch: 14 [50/100 (50%)]\tLoss: 0.001075\n",
      "Train Epoch: 14 [60/100 (60%)]\tLoss: 0.038739\n",
      "Train Epoch: 14 [70/100 (70%)]\tLoss: 0.038500\n",
      "Train Epoch: 14 [80/100 (80%)]\tLoss: 0.037887\n",
      "Train Epoch: 14 [90/100 (90%)]\tLoss: 0.021637\n",
      "\n",
      "Test set: \n",
      "Average loss: 0.2730, \n",
      "Accuracy: 10/20 (50.00%)\n",
      "\n",
      "Train Epoch: 15 [0/100 (0%)]\tLoss: 0.019307\n",
      "Train Epoch: 15 [10/100 (10%)]\tLoss: 0.000236\n",
      "Train Epoch: 15 [20/100 (20%)]\tLoss: 0.018010\n",
      "Train Epoch: 15 [30/100 (30%)]\tLoss: 0.018193\n",
      "Train Epoch: 15 [40/100 (40%)]\tLoss: 0.018107\n",
      "Train Epoch: 15 [50/100 (50%)]\tLoss: 0.035071\n",
      "Train Epoch: 15 [60/100 (60%)]\tLoss: 0.034872\n",
      "Train Epoch: 15 [70/100 (70%)]\tLoss: 0.003672\n",
      "Train Epoch: 15 [80/100 (80%)]\tLoss: 0.000825\n",
      "Train Epoch: 15 [90/100 (90%)]\tLoss: 0.033905\n",
      "\n",
      "Test set: \n",
      "Average loss: 0.2792, \n",
      "Accuracy: 10/20 (50.00%)\n",
      "\n",
      "Train Epoch: 16 [0/100 (0%)]\tLoss: 0.033952\n",
      "Train Epoch: 16 [10/100 (10%)]\tLoss: 0.033485\n",
      "Train Epoch: 16 [20/100 (20%)]\tLoss: 0.000114\n",
      "Train Epoch: 16 [30/100 (30%)]\tLoss: 0.000944\n",
      "Train Epoch: 16 [40/100 (40%)]\tLoss: 0.018716\n",
      "Train Epoch: 16 [50/100 (50%)]\tLoss: 0.000617\n",
      "Train Epoch: 16 [60/100 (60%)]\tLoss: 0.000076\n",
      "Train Epoch: 16 [70/100 (70%)]\tLoss: 0.032350\n",
      "Train Epoch: 16 [80/100 (80%)]\tLoss: 0.032046\n",
      "Train Epoch: 16 [90/100 (90%)]\tLoss: 0.015732\n",
      "\n",
      "Test set: \n",
      "Average loss: 0.2829, \n",
      "Accuracy: 10/20 (50.00%)\n",
      "\n",
      "Train Epoch: 17 [0/100 (0%)]\tLoss: 0.047100\n",
      "Train Epoch: 17 [10/100 (10%)]\tLoss: 0.015626\n",
      "Train Epoch: 17 [20/100 (20%)]\tLoss: 0.000561\n",
      "Train Epoch: 17 [30/100 (30%)]\tLoss: 0.000520\n",
      "Train Epoch: 17 [40/100 (40%)]\tLoss: 0.014994\n",
      "Train Epoch: 17 [50/100 (50%)]\tLoss: 0.044237\n",
      "Train Epoch: 17 [60/100 (60%)]\tLoss: 0.014658\n",
      "Train Epoch: 17 [70/100 (70%)]\tLoss: 0.017216\n",
      "Train Epoch: 17 [80/100 (80%)]\tLoss: 0.000475\n",
      "Train Epoch: 17 [90/100 (90%)]\tLoss: 0.000135\n",
      "\n",
      "Test set: \n",
      "Average loss: 0.2853, \n",
      "Accuracy: 10/20 (50.00%)\n",
      "\n",
      "Train Epoch: 18 [0/100 (0%)]\tLoss: 0.029033\n",
      "Train Epoch: 18 [10/100 (10%)]\tLoss: 0.030363\n",
      "Train Epoch: 18 [20/100 (20%)]\tLoss: 0.013891\n",
      "Train Epoch: 18 [30/100 (30%)]\tLoss: 0.013905\n",
      "Train Epoch: 18 [40/100 (40%)]\tLoss: 0.013652\n",
      "Train Epoch: 18 [50/100 (50%)]\tLoss: 0.000220\n",
      "Train Epoch: 18 [60/100 (60%)]\tLoss: 0.000160\n",
      "Train Epoch: 18 [70/100 (70%)]\tLoss: 0.000598\n",
      "Train Epoch: 18 [80/100 (80%)]\tLoss: 0.000634\n",
      "Train Epoch: 18 [90/100 (90%)]\tLoss: 0.040781\n",
      "\n",
      "Test set: \n",
      "Average loss: 0.2897, \n",
      "Accuracy: 10/20 (50.00%)\n",
      "\n",
      "Train Epoch: 19 [0/100 (0%)]\tLoss: 0.000583\n",
      "Train Epoch: 19 [10/100 (10%)]\tLoss: 0.013237\n",
      "Train Epoch: 19 [20/100 (20%)]\tLoss: 0.026340\n",
      "Train Epoch: 19 [30/100 (30%)]\tLoss: 0.026243\n",
      "Train Epoch: 19 [40/100 (40%)]\tLoss: 0.000067\n",
      "Train Epoch: 19 [50/100 (50%)]\tLoss: 0.025609\n",
      "Train Epoch: 19 [60/100 (60%)]\tLoss: 0.027531\n",
      "Train Epoch: 19 [70/100 (70%)]\tLoss: 0.012794\n",
      "Train Epoch: 19 [80/100 (80%)]\tLoss: 0.000610\n",
      "Train Epoch: 19 [90/100 (90%)]\tLoss: 0.000219\n",
      "\n",
      "Test set: \n",
      "Average loss: 0.2915, \n",
      "Accuracy: 10/20 (50.00%)\n",
      "\n",
      "Train Epoch: 20 [0/100 (0%)]\tLoss: 0.024417\n",
      "Train Epoch: 20 [10/100 (10%)]\tLoss: 0.012115\n",
      "Train Epoch: 20 [20/100 (20%)]\tLoss: 0.012481\n",
      "Train Epoch: 20 [30/100 (30%)]\tLoss: 0.012111\n",
      "Train Epoch: 20 [40/100 (40%)]\tLoss: 0.012195\n",
      "Train Epoch: 20 [50/100 (50%)]\tLoss: 0.023823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 20 [60/100 (60%)]\tLoss: 0.011800\n",
      "Train Epoch: 20 [70/100 (70%)]\tLoss: 0.000060\n",
      "Train Epoch: 20 [80/100 (80%)]\tLoss: 0.011638\n",
      "Train Epoch: 20 [90/100 (90%)]\tLoss: 0.002844\n",
      "\n",
      "Test set: \n",
      "Average loss: 0.2937, \n",
      "Accuracy: 10/20 (50.00%)\n",
      "\n",
      "Train Epoch: 21 [0/100 (0%)]\tLoss: 0.011408\n",
      "Train Epoch: 21 [10/100 (10%)]\tLoss: 0.000539\n",
      "Train Epoch: 21 [20/100 (20%)]\tLoss: 0.011557\n",
      "Train Epoch: 21 [30/100 (30%)]\tLoss: 0.000478\n",
      "Train Epoch: 21 [40/100 (40%)]\tLoss: 0.022474\n",
      "Train Epoch: 21 [50/100 (50%)]\tLoss: 0.011297\n",
      "Train Epoch: 21 [60/100 (60%)]\tLoss: 0.013167\n",
      "Train Epoch: 21 [70/100 (70%)]\tLoss: 0.011120\n",
      "Train Epoch: 21 [80/100 (80%)]\tLoss: 0.000361\n",
      "Train Epoch: 21 [90/100 (90%)]\tLoss: 0.032570\n",
      "\n",
      "Test set: \n",
      "Average loss: 0.2990, \n",
      "Accuracy: 10/20 (50.00%)\n",
      "\n",
      "Train Epoch: 22 [0/100 (0%)]\tLoss: 0.032351\n",
      "Train Epoch: 22 [10/100 (10%)]\tLoss: 0.012618\n",
      "Train Epoch: 22 [20/100 (20%)]\tLoss: 0.010520\n",
      "Train Epoch: 22 [30/100 (30%)]\tLoss: 0.000148\n",
      "Train Epoch: 22 [40/100 (40%)]\tLoss: 0.010648\n",
      "Train Epoch: 22 [50/100 (50%)]\tLoss: 0.000172\n",
      "Train Epoch: 22 [60/100 (60%)]\tLoss: 0.000316\n",
      "Train Epoch: 22 [70/100 (70%)]\tLoss: 0.030656\n",
      "Train Epoch: 22 [80/100 (80%)]\tLoss: 0.000348\n",
      "Train Epoch: 22 [90/100 (90%)]\tLoss: 0.010094\n",
      "\n",
      "Test set: \n",
      "Average loss: 0.3009, \n",
      "Accuracy: 10/20 (50.00%)\n",
      "\n",
      "Train Epoch: 23 [0/100 (0%)]\tLoss: 0.010212\n",
      "Train Epoch: 23 [10/100 (10%)]\tLoss: 0.000156\n",
      "Train Epoch: 23 [20/100 (20%)]\tLoss: 0.000025\n",
      "Train Epoch: 23 [30/100 (30%)]\tLoss: 0.020043\n",
      "Train Epoch: 23 [40/100 (40%)]\tLoss: 0.009797\n",
      "Train Epoch: 23 [50/100 (50%)]\tLoss: 0.009815\n",
      "Train Epoch: 23 [60/100 (60%)]\tLoss: 0.021519\n",
      "Train Epoch: 23 [70/100 (70%)]\tLoss: 0.019351\n",
      "Train Epoch: 23 [80/100 (80%)]\tLoss: 0.009483\n",
      "Train Epoch: 23 [90/100 (90%)]\tLoss: 0.000232\n",
      "\n",
      "Test set: \n",
      "Average loss: 0.3032, \n",
      "Accuracy: 10/20 (50.00%)\n",
      "\n",
      "Train Epoch: 24 [0/100 (0%)]\tLoss: 0.018864\n",
      "Train Epoch: 24 [10/100 (10%)]\tLoss: 0.009527\n",
      "Train Epoch: 24 [20/100 (20%)]\tLoss: 0.000240\n",
      "Train Epoch: 24 [30/100 (30%)]\tLoss: 0.009191\n",
      "Train Epoch: 24 [40/100 (40%)]\tLoss: 0.000224\n",
      "Train Epoch: 24 [50/100 (50%)]\tLoss: 0.027538\n",
      "Train Epoch: 24 [60/100 (60%)]\tLoss: 0.009079\n",
      "Train Epoch: 24 [70/100 (70%)]\tLoss: 0.009003\n",
      "Train Epoch: 24 [80/100 (80%)]\tLoss: 0.000231\n",
      "Train Epoch: 24 [90/100 (90%)]\tLoss: 0.010389\n",
      "\n",
      "Test set: \n",
      "Average loss: 0.3047, \n",
      "Accuracy: 10/20 (50.00%)\n",
      "\n",
      "Train Epoch: 25 [0/100 (0%)]\tLoss: 0.000016\n",
      "Train Epoch: 25 [10/100 (10%)]\tLoss: 0.000167\n",
      "Train Epoch: 25 [20/100 (20%)]\tLoss: 0.001564\n",
      "Train Epoch: 25 [30/100 (30%)]\tLoss: 0.008910\n",
      "Train Epoch: 25 [40/100 (40%)]\tLoss: 0.017418\n",
      "Train Epoch: 25 [50/100 (50%)]\tLoss: 0.017491\n",
      "Train Epoch: 25 [60/100 (60%)]\tLoss: 0.008861\n",
      "Train Epoch: 25 [70/100 (70%)]\tLoss: 0.008832\n",
      "Train Epoch: 25 [80/100 (80%)]\tLoss: 0.008530\n",
      "Train Epoch: 25 [90/100 (90%)]\tLoss: 0.016773\n",
      "\n",
      "Test set: \n",
      "Average loss: 0.3085, \n",
      "Accuracy: 10/20 (50.00%)\n",
      "\n",
      "Train Epoch: 26 [0/100 (0%)]\tLoss: 0.025244\n",
      "Train Epoch: 26 [10/100 (10%)]\tLoss: 0.001493\n",
      "Train Epoch: 26 [20/100 (20%)]\tLoss: 0.000275\n",
      "Train Epoch: 26 [30/100 (30%)]\tLoss: 0.024248\n",
      "Train Epoch: 26 [40/100 (40%)]\tLoss: 0.008006\n",
      "Train Epoch: 26 [50/100 (50%)]\tLoss: 0.000143\n",
      "Train Epoch: 26 [60/100 (60%)]\tLoss: 0.008021\n",
      "Train Epoch: 26 [70/100 (70%)]\tLoss: 0.007937\n",
      "Train Epoch: 26 [80/100 (80%)]\tLoss: 0.000139\n",
      "Train Epoch: 26 [90/100 (90%)]\tLoss: 0.007882\n",
      "\n",
      "Test set: \n",
      "Average loss: 0.3104, \n",
      "Accuracy: 10/20 (50.00%)\n",
      "\n",
      "Train Epoch: 27 [0/100 (0%)]\tLoss: 0.000083\n",
      "Train Epoch: 27 [10/100 (10%)]\tLoss: 0.007966\n",
      "Train Epoch: 27 [20/100 (20%)]\tLoss: 0.007829\n",
      "Train Epoch: 27 [30/100 (30%)]\tLoss: 0.007685\n",
      "Train Epoch: 27 [40/100 (40%)]\tLoss: 0.000316\n",
      "Train Epoch: 27 [50/100 (50%)]\tLoss: 0.024379\n",
      "Train Epoch: 27 [60/100 (60%)]\tLoss: 0.007638\n",
      "Train Epoch: 27 [70/100 (70%)]\tLoss: 0.014979\n",
      "Train Epoch: 27 [80/100 (80%)]\tLoss: 0.007439\n",
      "Train Epoch: 27 [90/100 (90%)]\tLoss: 0.000083\n",
      "\n",
      "Test set: \n",
      "Average loss: 0.3124, \n",
      "Accuracy: 10/20 (50.00%)\n",
      "\n",
      "Train Epoch: 28 [0/100 (0%)]\tLoss: 0.014653\n",
      "Train Epoch: 28 [10/100 (10%)]\tLoss: 0.007310\n",
      "Train Epoch: 28 [20/100 (20%)]\tLoss: 0.007383\n",
      "Train Epoch: 28 [30/100 (30%)]\tLoss: 0.000088\n",
      "Train Epoch: 28 [40/100 (40%)]\tLoss: 0.007179\n",
      "Train Epoch: 28 [50/100 (50%)]\tLoss: 0.000090\n",
      "Train Epoch: 28 [60/100 (60%)]\tLoss: 0.014375\n",
      "Train Epoch: 28 [70/100 (70%)]\tLoss: 0.014135\n",
      "Train Epoch: 28 [80/100 (80%)]\tLoss: 0.000258\n",
      "Train Epoch: 28 [90/100 (90%)]\tLoss: 0.008414\n",
      "\n",
      "Test set: \n",
      "Average loss: 0.3132, \n",
      "Accuracy: 10/20 (50.00%)\n",
      "\n",
      "Train Epoch: 29 [0/100 (0%)]\tLoss: 0.020830\n",
      "Train Epoch: 29 [10/100 (10%)]\tLoss: 0.000084\n",
      "Train Epoch: 29 [20/100 (20%)]\tLoss: 0.000005\n",
      "Train Epoch: 29 [30/100 (30%)]\tLoss: 0.000022\n",
      "Train Epoch: 29 [40/100 (40%)]\tLoss: 0.006895\n",
      "Train Epoch: 29 [50/100 (50%)]\tLoss: 0.006898\n",
      "Train Epoch: 29 [60/100 (60%)]\tLoss: 0.013570\n",
      "Train Epoch: 29 [70/100 (70%)]\tLoss: 0.001247\n",
      "Train Epoch: 29 [80/100 (80%)]\tLoss: 0.006989\n",
      "Train Epoch: 29 [90/100 (90%)]\tLoss: 0.013314\n",
      "\n",
      "Test set: \n",
      "Average loss: 0.3171, \n",
      "Accuracy: 10/20 (50.00%)\n",
      "\n",
      "Train Epoch: 30 [0/100 (0%)]\tLoss: 0.007770\n",
      "Train Epoch: 30 [10/100 (10%)]\tLoss: 0.000145\n",
      "Train Epoch: 30 [20/100 (20%)]\tLoss: 0.000025\n",
      "Train Epoch: 30 [30/100 (30%)]\tLoss: 0.000148\n",
      "Train Epoch: 30 [40/100 (40%)]\tLoss: 0.000114\n",
      "Train Epoch: 30 [50/100 (50%)]\tLoss: 0.013147\n",
      "Train Epoch: 30 [60/100 (60%)]\tLoss: 0.006650\n",
      "Train Epoch: 30 [70/100 (70%)]\tLoss: 0.019261\n",
      "Train Epoch: 30 [80/100 (80%)]\tLoss: 0.012741\n",
      "Train Epoch: 30 [90/100 (90%)]\tLoss: 0.006263\n",
      "\n",
      "Test set: \n",
      "Average loss: 0.3190, \n",
      "Accuracy: 10/20 (50.00%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Deep().to(device)\n",
    "\n",
    "# D√©finir l'optimiseur et la fonction de perte\n",
    "optimizer = optim.SGD(model.parameters(), lr=model_args['lr'], momentum=model_args['momentum'])\n",
    "class_weights = torch.tensor([1, 1], dtype=torch.float).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Fonction d'entra√Ænement\n",
    "def train(model, train_loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % model_args['log_interval'] == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "# Fonction de test\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            pred = pred.squeeze()\n",
    "            # R√©initialisation des variables\n",
    "           \n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    \n",
    "    print('\\nTest set: \\nAverage loss: {:.4f}, \\nAccuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset), accuracy))\n",
    "\n",
    "# Training\n",
    "for epoch in range(1, model_args['epochs'] + 1):\n",
    "    train(model, train_loader, optimizer, criterion, epoch)\n",
    "    test(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
